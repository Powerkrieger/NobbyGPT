{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spellchecker-Utility for testing the generated TXTs\n",
    "\n",
    "These Code-Blocks can be used to determine the following characteristigs of a .txt file:\n",
    "\n",
    "1. Spelling and grammar mistakes combined (count and %-share)\n",
    "2. Vocabulary size (number of different words) for all words and german only\n",
    "3. Vocabulary frequency (histogram of used words) for all words and german only\n",
    "\n",
    "\n",
    "\n",
    "To use this script, simply upload a .txt file, specify its name in the \"filename\" variable and run the notebook.\n",
    "\n",
    "*Thanks to \"Marco Polo\" for aiding us with those python-libraries :)*"
   ],
   "metadata": {
    "id": "z5VbYmP5dBZy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Install all dependencies**"
   ],
   "metadata": {
    "id": "BMvrABnnnnuz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mfpSCNVlc8Da",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d319e1a7-254d-4d11-aaf0-11f96e72e96b",
    "ExecuteTime": {
     "end_time": "2024-01-23T12:05:40.472383700Z",
     "start_time": "2024-01-23T12:05:25.845419600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: language_tool_python in c:\\users\\jorehder\\gitrepos\\nobbygpt\\venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jorehder\\gitrepos\\nobbygpt\\venv\\lib\\site-packages (from language_tool_python) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jorehder\\gitrepos\\nobbygpt\\venv\\lib\\site-packages (from language_tool_python) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jorehder\\gitrepos\\nobbygpt\\venv\\lib\\site-packages (from requests->language_tool_python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jorehder\\gitrepos\\nobbygpt\\venv\\lib\\site-packages (from requests->language_tool_python) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jorehder\\gitrepos\\nobbygpt\\venv\\lib\\site-packages (from requests->language_tool_python) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jorehder\\gitrepos\\nobbygpt\\venv\\lib\\site-packages (from requests->language_tool_python) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\jorehder\\gitrepos\\nobbygpt\\venv\\lib\\site-packages (from tqdm->language_tool_python) (0.4.6)\n",
      "Requirement already satisfied: pyspellchecker in c:\\users\\jorehder\\gitrepos\\nobbygpt\\venv\\lib\\site-packages (0.8.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"apt\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hunspell\n",
      "  Using cached hunspell-0.5.5.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: hunspell\n",
      "  Building wheel for hunspell (setup.py): started\n",
      "  Building wheel for hunspell (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for hunspell\n",
      "Failed to build hunspell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [5 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'hunspell' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hunspell\n",
      "ERROR: Could not build wheels for hunspell, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "\n",
    "!pip install language_tool_python\n",
    "!pip install pyspellchecker\n",
    "!apt install build-essential python3-dev libhunspell-dev\n",
    "!pip install hunspell\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Import all needed libraries**"
   ],
   "metadata": {
    "id": "2d_8fAXfntoL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import language_tool_python\n",
    "from spellchecker import SpellChecker\n",
    "import hunspell\n",
    "import nltk"
   ],
   "metadata": {
    "id": "vQ948GR3gLwq",
    "ExecuteTime": {
     "end_time": "2024-01-23T12:14:58.764642Z",
     "start_time": "2024-01-23T12:14:57.339083900Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hunspell'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlanguage_tool_python\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspellchecker\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SpellChecker\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mhunspell\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'hunspell'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Run the preconfiguration**\n",
    "- Please specify the name of the .txt file to check in this cell\n",
    "- If you want to change the preview-sizes, you can change them here as well\n",
    "- Plese dont forget to upload the needed files for hunspell into the runtime as stated in the comment below\n",
    "- If you, for whatever reason, need to change the parameters of the spellchecking libraries, please use this cell to do so"
   ],
   "metadata": {
    "id": "Fwj-p212n0Ce"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuration variables -> Change things here!\n",
    "\n",
    "tool = language_tool_python.LanguageTool('de-DE', config={ 'cacheSize': 1000, 'pipelineCaching': True, 'maxSpellingSuggestions': 1 }) # LanguageTool Setup\n",
    "spell = SpellChecker(language='de') # PySpellChecker Setup\n",
    "\n",
    "# For HunSpell you will need files from here: https://github.com/elastic/hunspell/tree/master/dicts/de_DE\n",
    "d = hunspell.HunSpell(\"de_DE.dic\", \"de_DE.aff\") # Upload these two files from the provided GitHub URL into the instance!\n",
    "\n",
    "# Initialization of nltk\n",
    "nltk.download('words')\n",
    "eng_words = nltk.corpus.words.words()\n",
    "\n",
    "# Misc parameters\n",
    "text_preview_len = 250 # Length of the .txt preview\n",
    "vocab_hist_preview = 10 # Length of the vocabulary preview and german word preview\n",
    "filename = 'Test.txt' # Name of the file to check"
   ],
   "metadata": {
    "id": "Cq5GrijjgUf4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6c3fc283-f674-4c63-b234-322da1f2103a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading LanguageTool 5.7: 100%|██████████| 225M/225M [00:04<00:00, 45.3MB/s]\n",
      "INFO:language_tool_python.download_lt:Unzipping /tmp/tmpcqtd0k9e.zip to /root/.cache/language_tool_python.\n",
      "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-5.7.zip to /root/.cache/language_tool_python.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Read in the .txt file here**\n",
    "- There will be some stats and a short preview in the output"
   ],
   "metadata": {
    "id": "1aEcfj_HhzEv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Reading the textfile and previewing the first n characters\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "  text = f.read()\n",
    "\n",
    "words = text.split()\n",
    "words = [re.sub(r'[^a-zA-ZßäöüÄÖÜ]', '', word) for word in words]\n",
    "\n",
    "print(\"Lenth of file (words): \", len(words))\n",
    "print(\"Lenth of file (chars): \", len(text), \"\\n\")\n",
    "print(text[:text_preview_len])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oIzj8mth3Fe",
    "outputId": "48e7c26e-816e-4255-eb72-ce10605eaaee"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lenth of file (words):  8\n",
      "Lenth of file (chars):  53 \n",
      "\n",
      "Hallo hallo, this Datei enthält two englische Wörter.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**5. Check the .txt files with different spellchecking-tools**\n",
    "- In the following cells you will find code for LanguageTool, PySpellChecker and HunSpell\n",
    "- The reason for this are the different vocabulary-lists and checking-scripts which the libraries use -> together they hopefully produce a somewhat clean result"
   ],
   "metadata": {
    "id": "Qvb_GGezo1gm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Check with LanguageTool\n",
    "\n",
    "matches = tool.check(text)\n",
    "print(\"LanguageTool analysis:\")\n",
    "print(\"Number of spelling mistakes: \", len(matches))\n",
    "print(\"Error rate: \", (len(matches) / len(words)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edBX62Sno02M",
    "outputId": "dea0745e-3a06-46e2-8197-fa3ad52cc75b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LanguageTool analysis:\n",
      "Number of spelling mistakes:  3\n",
      "Error rate:  0.375\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check with PySpellChecker\n",
    "misspelled = spell.unknown(words)\n",
    "print(\"PySpellChecker analysis:\")\n",
    "print(\"Misspelled Words: \", len(misspelled))\n",
    "print(\"Error Rate: \", (len(misspelled) / len(words)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J05matI8-lH1",
    "outputId": "93eceb58-74b7-4e39-a5b7-85ea563bba48"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PySpellChecker analysis:\n",
      "Misspelled Words:  2\n",
      "Error Rate:  0.25\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check with Hunspell\n",
    "\n",
    "errors = []\n",
    "for word in words:\n",
    "    if not d.spell(word):\n",
    "        errors.append(word)\n",
    "\n",
    "print(\"HunSpell analysis:\")\n",
    "print(\"Misspelled Words: \", len(errors))\n",
    "print(\"Error Rate: \", (len(errors) / len(words)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xd7Ar1m8Bfn_",
    "outputId": "fad0a6d5-1323-409f-d4d3-4239a22494a5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HunSpell analysis:\n",
      "Misspelled Words:  2\n",
      "Error Rate:  0.25\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**6. Gather information about the vocabulary**\n",
    "- The following cell will analyze the spectrum and frequency of the used vocabulary\n",
    "- This code considers all words, german and english"
   ],
   "metadata": {
    "id": "SMcogJ-7peKb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Build vocabulary and print size\n",
    "\n",
    "words_v = [word.lower() for word in words]\n",
    "vocabulary_dict = dict.fromkeys(words_v)\n",
    "vocabulary = list(vocabulary_dict)\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"\\nThe \" + str(vocab_hist_preview) + \" most used words:\")\n",
    "\n",
    "# Build histogram from vocabulary and preview the n most used words\n",
    "\n",
    "vocab_hist = []\n",
    "for word in set(words_v):\n",
    "  count = words_v.count(word)\n",
    "  elem = (word, count)\n",
    "  vocab_hist.append(elem)\n",
    "\n",
    "vocab_hist.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for word, count in vocab_hist[:vocab_hist_preview]:\n",
    "  print(f\"{word}: {count}\")"
   ],
   "metadata": {
    "id": "KPcOp-a9AAEp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ea6cf3bc-a732-4ed2-88d5-aedf69886764"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 7\n",
      "\n",
      "The 10 most used words:\n",
      "hallo: 2\n",
      "two: 1\n",
      "datei: 1\n",
      "englische: 1\n",
      "this: 1\n",
      "enthält: 1\n",
      "wörter: 1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**7. Gather information about the german vocabulary**\n",
    "\n",
    "- Now we will analyze only the german words in the .txt file\n",
    "- To extract them, an english word-list by nltk is used\n",
    "- This script will output the german words in the .txt file as well as german vocabulary characteristics as seen above"
   ],
   "metadata": {
    "id": "uRWa1oDwqCPX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# German words and german vocabulary\n",
    "\n",
    "ger_words = [] # for building the vocabulary later\n",
    "for word in words:\n",
    "  if word not in eng_words:\n",
    "    ger_words.append(word)\n",
    "\n",
    "ger_words_tr = [] # for the correct text output (inefficient, but whatever :)\n",
    "for voc in vocabulary:\n",
    "  if voc not in eng_words:\n",
    "    ger_words_tr.append(voc)\n",
    "\n",
    "print(\"Number of german words, according to nltk:\", len(ger_words))\n",
    "print(\"German word rate:\", (len(ger_words) / len(words)))\n",
    "print(\"\\nThe \" + str(vocab_hist_preview) + \" first german words:\")\n",
    "for entry in ger_words_tr[:vocab_hist_preview]:\n",
    "  print(entry)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Build german vocabulary and print size\n",
    "\n",
    "words_ger = [word.lower() for word in ger_words]\n",
    "vocabulary_dict_ger = dict.fromkeys(words_ger)\n",
    "vocabulary_ger = list(vocabulary_dict_ger)\n",
    "print(\"German vocabulary size:\", len(vocabulary_ger))\n",
    "print(\"\\nThe \" + str(vocab_hist_preview) + \" most used german words:\")\n",
    "\n",
    "# Build german histogram from vocabulary and preview the n most used words\n",
    "\n",
    "vocab_hist_ger = []\n",
    "for word in set(words_ger):\n",
    "  count = words_ger.count(word)\n",
    "  elem = (word, count)\n",
    "  vocab_hist_ger.append(elem)\n",
    "\n",
    "vocab_hist_ger.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for word, count in vocab_hist_ger[:vocab_hist_preview]:\n",
    "  print(f\"{word}: {count}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxi-SxWfgMDJ",
    "outputId": "16451b7a-ea57-4e6b-a9aa-d54298fdee0d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of german words, according to nltk: 6\n",
      "German word rate: 0.75\n",
      "\n",
      "The 10 first german words:\n",
      "hallo\n",
      "datei\n",
      "enthält\n",
      "englische\n",
      "wörter\n",
      "\n",
      "\n",
      "German vocabulary size: 5\n",
      "\n",
      "The 10 most used german words:\n",
      "hallo: 2\n",
      "datei: 1\n",
      "englische: 1\n",
      "enthält: 1\n",
      "wörter: 1\n"
     ]
    }
   ]
  }
 ]
}
