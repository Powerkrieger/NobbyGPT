{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spellchecker-Utility for testing the generated TXTs\n",
        "\n",
        "These Code-Blocks can be used to determine the following characteristigs of a .txt file:\n",
        "\n",
        "1. Spelling and grammar mistakes combined (count and %-share)\n",
        "2. Vocabulary size (number of different words) for all words and german only\n",
        "3. Vocabulary frequency (histogram of used words) for all words and german only\n",
        "\n",
        "\n",
        "\n",
        "To use this script, simply upload a .txt file, specify its name in the \"filename\" variable and run the notebook.\n",
        "\n",
        "*Thanks to \"Marco Polo\" for aiding us with those python-libraries :)*"
      ],
      "metadata": {
        "id": "z5VbYmP5dBZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Install all dependencies**"
      ],
      "metadata": {
        "id": "BMvrABnnnnuz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfpSCNVlc8Da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d319e1a7-254d-4d11-aaf0-11f96e72e96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2023.11.17)\n",
            "Installing collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.7.1\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.3-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.3\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04).\n",
            "python3-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  dictionaries-common hunspell-en-us libhunspell-1.7-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  ispell | aspell | hunspell wordlist hunspell openoffice.org-hunspell | openoffice.org-core\n",
            "The following NEW packages will be installed:\n",
            "  dictionaries-common hunspell-en-us libhunspell-1.7-0 libhunspell-dev libtext-iconv-perl\n",
            "0 upgraded, 5 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 896 kB of archives.\n",
            "After this operation, 3,130 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtext-iconv-perl amd64 1.7-7build3 [14.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 dictionaries-common all 1.28.14 [185 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-en-us all 1:2020.12.07-2 [280 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-1.7-0 amd64 1.7.0-4build1 [175 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-dev amd64 1.7.0-4build1 [241 kB]\n",
            "Fetched 896 kB in 0s (2,336 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 121654 files and directories currently installed.)\n",
            "Preparing to unpack .../libtext-iconv-perl_1.7-7build3_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-7build3) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../dictionaries-common_1.28.14_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.28.14) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../hunspell-en-us_1%3a2020.12.07-2_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2020.12.07-2) ...\n",
            "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
            "Preparing to unpack .../libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Selecting previously unselected package libhunspell-dev:amd64.\n",
            "Preparing to unpack .../libhunspell-dev_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-dev:amd64 (1.7.0-4build1) ...\n",
            "Setting up libtext-iconv-perl (1.7-7build3) ...\n",
            "Setting up dictionaries-common (1.28.14) ...\n",
            "Setting up hunspell-en-us (1:2020.12.07-2) ...\n",
            "Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Setting up libhunspell-dev:amd64 (1.7.0-4build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for dictionaries-common (1.28.14) ...\n",
            "Collecting hunspell\n",
            "  Downloading hunspell-0.5.5.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: hunspell\n",
            "  Building wheel for hunspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hunspell: filename=hunspell-0.5.5-cp310-cp310-linux_x86_64.whl size=66271 sha256=c7310bfdb18086da3cac33d358a44da3929903517392b150e6fd0de594810a68\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/f3/bd/bdce223532ee8aa5345e14e0d6e7ba06cbbaff8767cefe1ec8\n",
            "Successfully built hunspell\n",
            "Installing collected packages: hunspell\n",
            "Successfully installed hunspell-0.5.5\n"
          ]
        }
      ],
      "source": [
        "# Dependencies\n",
        "\n",
        "!pip install language_tool_python\n",
        "!pip install pyspellchecker\n",
        "!apt install build-essential python3-dev libhunspell-dev\n",
        "!pip install hunspell"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Import all needed libraries**"
      ],
      "metadata": {
        "id": "2d_8fAXfntoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import language_tool_python\n",
        "from spellchecker import SpellChecker\n",
        "import hunspell\n",
        "import nltk"
      ],
      "metadata": {
        "id": "vQ948GR3gLwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Run the preconfiguration**\n",
        "- Please specify the name of the .txt file to check in this cell\n",
        "- If you want to change the preview-sizes, you can change them here as well\n",
        "- Plese dont forget to upload the needed files for hunspell into the runtime as stated in the comment below\n",
        "- If you, for whatever reason, need to change the parameters of the spellchecking libraries, please use this cell to do so"
      ],
      "metadata": {
        "id": "Fwj-p212n0Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration variables -> Change things here!\n",
        "\n",
        "tool = language_tool_python.LanguageTool('de-DE', config={ 'cacheSize': 1000, 'pipelineCaching': True, 'maxSpellingSuggestions': 1 }) # LanguageTool Setup\n",
        "spell = SpellChecker(language='de') # PySpellChecker Setup\n",
        "\n",
        "# For HunSpell you will need files from here: https://github.com/elastic/hunspell/tree/master/dicts/de_DE\n",
        "d = hunspell.HunSpell(\"de_DE.dic\", \"de_DE.aff\") # Upload these two files from the provided GitHub URL into the instance!\n",
        "\n",
        "# Initialization of nltk\n",
        "nltk.download('words')\n",
        "eng_words = nltk.corpus.words.words()\n",
        "\n",
        "# Misc parameters\n",
        "text_preview_len = 250 # Length of the .txt preview\n",
        "vocab_hist_preview = 10 # Length of the vocabulary preview and german word preview\n",
        "filename = 'Test.txt' # Name of the file to check"
      ],
      "metadata": {
        "id": "Cq5GrijjgUf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3fc283-f674-4c63-b234-322da1f2103a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading LanguageTool 5.7: 100%|██████████| 225M/225M [00:04<00:00, 45.3MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmpcqtd0k9e.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-5.7.zip to /root/.cache/language_tool_python.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Read in the .txt file here**\n",
        "- There will be some stats and a short preview in the output"
      ],
      "metadata": {
        "id": "1aEcfj_HhzEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the textfile and previewing the first n characters\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "words = text.split()\n",
        "words = [re.sub(r'[^a-zA-ZßäöüÄÖÜ]', '', word) for word in words]\n",
        "\n",
        "print(\"Lenth of file (words): \", len(words))\n",
        "print(\"Lenth of file (chars): \", len(text), \"\\n\")\n",
        "print(text[:text_preview_len])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oIzj8mth3Fe",
        "outputId": "48e7c26e-816e-4255-eb72-ce10605eaaee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenth of file (words):  8\n",
            "Lenth of file (chars):  53 \n",
            "\n",
            "Hallo hallo, this Datei enthält two englische Wörter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Check the .txt files with different spellchecking-tools**\n",
        "- In the following cells you will find code for LanguageTool, PySpellChecker and HunSpell\n",
        "- The reason for this are the different vocabulary-lists and checking-scripts which the libraries use -> together they hopefully produce a somewhat clean result"
      ],
      "metadata": {
        "id": "Qvb_GGezo1gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check with LanguageTool\n",
        "\n",
        "matches = tool.check(text)\n",
        "print(\"LanguageTool analysis:\")\n",
        "print(\"Number of spelling mistakes: \", len(matches))\n",
        "print(\"Error rate: \", (len(matches) / len(words)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edBX62Sno02M",
        "outputId": "dea0745e-3a06-46e2-8197-fa3ad52cc75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LanguageTool analysis:\n",
            "Number of spelling mistakes:  3\n",
            "Error rate:  0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check with PySpellChecker\n",
        "misspelled = spell.unknown(words)\n",
        "print(\"PySpellChecker analysis:\")\n",
        "print(\"Misspelled Words: \", len(misspelled))\n",
        "print(\"Error Rate: \", (len(misspelled) / len(words)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J05matI8-lH1",
        "outputId": "93eceb58-74b7-4e39-a5b7-85ea563bba48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpellChecker analysis:\n",
            "Misspelled Words:  2\n",
            "Error Rate:  0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check with Hunspell\n",
        "\n",
        "errors = []\n",
        "for word in words:\n",
        "    if not d.spell(word):\n",
        "        errors.append(word)\n",
        "\n",
        "print(\"HunSpell analysis:\")\n",
        "print(\"Misspelled Words: \", len(errors))\n",
        "print(\"Error Rate: \", (len(errors) / len(words)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd7Ar1m8Bfn_",
        "outputId": "fad0a6d5-1323-409f-d4d3-4239a22494a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HunSpell analysis:\n",
            "Misspelled Words:  2\n",
            "Error Rate:  0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Gather information about the vocabulary**\n",
        "- The following cell will analyze the spectrum and frequency of the used vocabulary\n",
        "- This code considers all words, german and english"
      ],
      "metadata": {
        "id": "SMcogJ-7peKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary and print size\n",
        "\n",
        "words_v = [word.lower() for word in words]\n",
        "vocabulary_dict = dict.fromkeys(words_v)\n",
        "vocabulary = list(vocabulary_dict)\n",
        "print(\"Vocabulary size:\", len(vocabulary))\n",
        "print(\"\\nThe \" + str(vocab_hist_preview) + \" most used words:\")\n",
        "\n",
        "# Build histogram from vocabulary and preview the n most used words\n",
        "\n",
        "vocab_hist = []\n",
        "for word in set(words_v):\n",
        "  count = words_v.count(word)\n",
        "  elem = (word, count)\n",
        "  vocab_hist.append(elem)\n",
        "\n",
        "vocab_hist.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for word, count in vocab_hist[:vocab_hist_preview]:\n",
        "  print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "KPcOp-a9AAEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6cf3bc-a732-4ed2-88d5-aedf69886764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 7\n",
            "\n",
            "The 10 most used words:\n",
            "hallo: 2\n",
            "two: 1\n",
            "datei: 1\n",
            "englische: 1\n",
            "this: 1\n",
            "enthält: 1\n",
            "wörter: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Gather information about the german vocabulary**\n",
        "\n",
        "- Now we will analyze only the german words in the .txt file\n",
        "- To extract them, an english word-list by nltk is used\n",
        "- This script will output the german words in the .txt file as well as german vocabulary characteristics as seen above"
      ],
      "metadata": {
        "id": "uRWa1oDwqCPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# German words and german vocabulary\n",
        "\n",
        "ger_words = [] # for building the vocabulary later\n",
        "for word in words:\n",
        "  if word not in eng_words:\n",
        "    ger_words.append(word)\n",
        "\n",
        "ger_words_tr = [] # for the correct text output (inefficient, but whatever :)\n",
        "for voc in vocabulary:\n",
        "  if voc not in eng_words:\n",
        "    ger_words_tr.append(voc)\n",
        "\n",
        "print(\"Number of german words, according to nltk:\", len(ger_words))\n",
        "print(\"German word rate:\", (len(ger_words) / len(words)))\n",
        "print(\"\\nThe \" + str(vocab_hist_preview) + \" first german words:\")\n",
        "for entry in ger_words_tr[:vocab_hist_preview]:\n",
        "  print(entry)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Build german vocabulary and print size\n",
        "\n",
        "words_ger = [word.lower() for word in ger_words]\n",
        "vocabulary_dict_ger = dict.fromkeys(words_ger)\n",
        "vocabulary_ger = list(vocabulary_dict_ger)\n",
        "print(\"German vocabulary size:\", len(vocabulary_ger))\n",
        "print(\"\\nThe \" + str(vocab_hist_preview) + \" most used german words:\")\n",
        "\n",
        "# Build german histogram from vocabulary and preview the n most used words\n",
        "\n",
        "vocab_hist_ger = []\n",
        "for word in set(words_ger):\n",
        "  count = words_ger.count(word)\n",
        "  elem = (word, count)\n",
        "  vocab_hist_ger.append(elem)\n",
        "\n",
        "vocab_hist_ger.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for word, count in vocab_hist_ger[:vocab_hist_preview]:\n",
        "  print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxi-SxWfgMDJ",
        "outputId": "16451b7a-ea57-4e6b-a9aa-d54298fdee0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of german words, according to nltk: 6\n",
            "German word rate: 0.75\n",
            "\n",
            "The 10 first german words:\n",
            "hallo\n",
            "datei\n",
            "enthält\n",
            "englische\n",
            "wörter\n",
            "\n",
            "\n",
            "German vocabulary size: 5\n",
            "\n",
            "The 10 most used german words:\n",
            "hallo: 2\n",
            "datei: 1\n",
            "englische: 1\n",
            "enthält: 1\n",
            "wörter: 1\n"
          ]
        }
      ]
    }
  ]
}